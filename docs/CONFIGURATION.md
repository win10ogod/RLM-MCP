# RLM MCP Server Configuration Guide

## Environment Variables

### Required

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | API key for OpenAI or compatible API | - |

### Optional

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_BASE_URL` | API base URL | `https://api.openai.com/v1` |
| `RLM_MODEL` | Root model for RLM controller | `gpt-4o-mini` |
| `RLM_SUB_MODEL` | Sub-model for recursive calls | `gpt-4o-mini` |
| `PORT` | HTTP server port | `3000` |
| `RLM_STORAGE_DIR` | Enable persisted context storage | - |

## Using Different LLM Providers

### OpenAI

```bash
export OPENAI_API_KEY="sk-..."
export OPENAI_BASE_URL="https://api.openai.com/v1"
export RLM_MODEL="gpt-4o"
export RLM_SUB_MODEL="gpt-4o-mini"
```

### Azure OpenAI

```bash
export OPENAI_API_KEY="your-azure-api-key"
export OPENAI_BASE_URL="https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT"
export RLM_MODEL="gpt-4o"
export RLM_SUB_MODEL="gpt-4o-mini"
```

### Local LLM (Ollama)

```bash
export OPENAI_API_KEY="ollama"  # Any string works
export OPENAI_BASE_URL="http://localhost:11434/v1"
export RLM_MODEL="llama3.2"
export RLM_SUB_MODEL="llama3.2"
```

### Local LLM (llama.cpp server)

```bash
export OPENAI_API_KEY="local"  # Any string works
export OPENAI_BASE_URL="http://localhost:8080/v1"
export RLM_MODEL="local-model"
export RLM_SUB_MODEL="local-model"
```

### vLLM

```bash
export OPENAI_API_KEY="vllm"
export OPENAI_BASE_URL="http://localhost:8000/v1"
export RLM_MODEL="meta-llama/Llama-3-8b-instruct"
export RLM_SUB_MODEL="meta-llama/Llama-3-8b-instruct"
```

### Anthropic (via proxy)

If using an OpenAI-compatible proxy for Anthropic:

```bash
export OPENAI_API_KEY="your-anthropic-key"
export OPENAI_BASE_URL="http://localhost:3001/v1"  # Proxy URL
export RLM_MODEL="claude-3-5-sonnet"
export RLM_SUB_MODEL="claude-3-5-haiku"
```

## Model Selection Guide

### Root Model (RLM_MODEL)

The root model controls the RLM process:
- Analyzes context structure
- Plans decomposition strategy
- Writes code for REPL
- Aggregates results

**Recommendations:**
- Use a capable model with good code generation
- GPT-4o, Claude 3.5 Sonnet, or similar
- Balance between capability and cost

### Sub-Model (RLM_SUB_MODEL)

The sub-model handles recursive queries:
- Processes individual context chunks
- Answers specific questions
- Can be smaller/cheaper than root model

**Recommendations:**
- GPT-4o-mini, Claude 3.5 Haiku, or similar
- Lower latency is beneficial for batch calls
- Cost-effective for many parallel calls

## Transport Configuration

### Stdio Mode (Default)

Best for local MCP clients:

```bash
node dist/index.js
```

### HTTP Mode

Best for remote access or multiple clients:

```bash
node dist/index.js --http --port=3000
```

HTTP Endpoints:
- `POST /mcp` - MCP protocol
- `GET /health` - Health check
- `GET /info` - Server information

## Resource Limits

### Iteration Limits

```typescript
{
  max_iterations: 20,     // Default
  max_iterations: 50,     // Maximum allowed
}
```

Higher iterations allow more complex reasoning but increase:
- Processing time
- Token usage
- Cost

### Timeout Configuration

```typescript
{
  timeout_ms: 300000,     // Default: 5 minutes
  timeout_ms: 600000,     // Maximum: 10 minutes
}
```

### Output Limits

- `CHARACTER_LIMIT`: 50,000 characters (response truncation)
- `MAX_OUTPUT_LENGTH`: 100,000 characters
- `MAX_REPL_OUTPUT`: 8,192 characters per REPL execution

## Performance Tuning

### For Speed

- Use smaller sub-model (`gpt-4o-mini`)
- Reduce `max_iterations`
- Use `chunk` strategy for simple tasks

### For Quality

- Use capable root model (`gpt-4o`)
- Allow more iterations
- Use `semantic` strategy for documents

### For Cost

- Use `gpt-4o-mini` for both models
- Set appropriate iteration limits
- Pre-filter context before processing

## Security Considerations

### API Key Protection

- Never commit API keys to version control
- Use environment variables or secrets management
- Rotate keys periodically

### Code Execution

The REPL environment executes JavaScript code generated by the LLM:
- Code runs in a sandboxed environment
- Network access is restricted
- File system access is disabled
- Timeout prevents infinite loops

### Input Validation

- All inputs are validated via Zod schemas
- Context size is not limited (RLM handles large inputs)
- Query length is limited to 10,000 characters
